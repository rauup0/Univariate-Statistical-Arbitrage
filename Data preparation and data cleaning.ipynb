{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ccbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# List of ticker symbols\n",
    "tickers = [\n",
    "    \"XLE\", \"XLF\", \"XLU\", \"XLI\", \"GDX\", \"XLK\", \"XLV\", \"XLY\", \"XLP\", \"XLB\",\n",
    "    \"XOP\", \"IYR\", \"XHB\", \"ITB\", \"VNQ\", \"GDXJ\", \"IYE\", \"OIH\", \"XME\", \"XRT\",\n",
    "    \"SMH\", \"IBB\", \"KBE\", \"KRE\", \"XTL\"\n",
    "]\n",
    "\n",
    "# Define start date\n",
    "start_date = \"1990-01-01\"\n",
    "# Get end date (current date)\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "# Load data using yfinance\n",
    "ETFS = yf.download(tickers, start=start_date, end=end_date)[\"Adj Close\"]\n",
    "# Save the data to a CSV file\n",
    "ETFS.to_csv(\"ETFS.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef43553b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  4000 of 4000 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22 Failed downloads:\n",
      "['CWEN.A', 'HEI.A', 'NETD', 'AKO.B', 'BF.A', 'PBR.A', 'BH.A', 'AGM.A', 'KVAC', 'CRD.A', 'GTN.A', 'LGF.A', 'BRK.B', 'MOG.A', 'BRK.A', 'MOG.B', 'CRD.B', 'HYAC', 'AKO.A']: Exception('%ticker%: No timezone found, symbol may be delisted')\n",
      "['GEF.B', 'LEN.B', 'BF.B']: Exception('%ticker%: No price data found, symbol may be delisted (1d 2000-01-01 -> 2023-08-04)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tickers from Excel file\n",
    "ticker_file = r'C:\\Users\\rauup\\Desktop\\tfm\\tickers.xlsx'\n",
    "ticker_df = pd.read_excel(ticker_file, header=None)  # No header in the Excel file\n",
    "\n",
    "# Extract tickers from the DataFrame column\n",
    "tickers = ticker_df[0].tolist()\n",
    "\n",
    "# Define start date\n",
    "start_date = \"2000-01-01\"\n",
    "# Get end date (current date)\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "# Load data using yfinance\n",
    "fulldata = yf.download(tickers, start=start_date, end=end_date)[\"Adj Close\"]\n",
    "fulldata.to_csv(\"stocks_price_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c95ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fulldata = pd.read_csv('stocks_price_data.csv')\n",
    "etfdata = pd.read_csv('ETFS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c12a6dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2000-01-03'\n",
    "etfdata = etfdata[etfdata.index >= start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fab12c2d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AA</th>\n",
       "      <th>AAC</th>\n",
       "      <th>AACT</th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAN</th>\n",
       "      <th>AAOI</th>\n",
       "      <th>AAON</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>...</th>\n",
       "      <th>ZTO</th>\n",
       "      <th>ZTS</th>\n",
       "      <th>ZUMZ</th>\n",
       "      <th>ZUO</th>\n",
       "      <th>ZURA</th>\n",
       "      <th>ZVIA</th>\n",
       "      <th>ZVRA</th>\n",
       "      <th>ZWS</th>\n",
       "      <th>ZYME</th>\n",
       "      <th>ZYXI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>44.290768</td>\n",
       "      <td>71.203278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.859423</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>40.907444</td>\n",
       "      <td>71.533180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.039065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.786965</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>38.369949</td>\n",
       "      <td>75.656914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.048301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.798481</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>36.908974</td>\n",
       "      <td>74.667213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.052919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.729382</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>39.984726</td>\n",
       "      <td>74.447304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.015974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.763932</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-27</th>\n",
       "      <td>126.660004</td>\n",
       "      <td>33.910000</td>\n",
       "      <td>10.59</td>\n",
       "      <td>10.19</td>\n",
       "      <td>16.570000</td>\n",
       "      <td>15.38</td>\n",
       "      <td>6.19</td>\n",
       "      <td>100.019997</td>\n",
       "      <td>72.639999</td>\n",
       "      <td>193.220001</td>\n",
       "      <td>...</td>\n",
       "      <td>26.570000</td>\n",
       "      <td>188.690002</td>\n",
       "      <td>18.760000</td>\n",
       "      <td>10.51</td>\n",
       "      <td>6.57</td>\n",
       "      <td>2.72</td>\n",
       "      <td>4.64</td>\n",
       "      <td>29.790001</td>\n",
       "      <td>7.28</td>\n",
       "      <td>9.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-28</th>\n",
       "      <td>126.050003</td>\n",
       "      <td>34.759998</td>\n",
       "      <td>10.62</td>\n",
       "      <td>10.18</td>\n",
       "      <td>16.790001</td>\n",
       "      <td>15.43</td>\n",
       "      <td>6.71</td>\n",
       "      <td>103.019997</td>\n",
       "      <td>73.169998</td>\n",
       "      <td>195.830002</td>\n",
       "      <td>...</td>\n",
       "      <td>27.990000</td>\n",
       "      <td>189.899994</td>\n",
       "      <td>18.920000</td>\n",
       "      <td>10.91</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2.72</td>\n",
       "      <td>4.83</td>\n",
       "      <td>30.440001</td>\n",
       "      <td>7.27</td>\n",
       "      <td>9.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-31</th>\n",
       "      <td>121.769997</td>\n",
       "      <td>36.189999</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.19</td>\n",
       "      <td>16.750000</td>\n",
       "      <td>15.82</td>\n",
       "      <td>6.75</td>\n",
       "      <td>105.260002</td>\n",
       "      <td>74.389999</td>\n",
       "      <td>196.449997</td>\n",
       "      <td>...</td>\n",
       "      <td>27.799999</td>\n",
       "      <td>188.089996</td>\n",
       "      <td>18.860001</td>\n",
       "      <td>11.73</td>\n",
       "      <td>6.88</td>\n",
       "      <td>2.78</td>\n",
       "      <td>4.88</td>\n",
       "      <td>30.440001</td>\n",
       "      <td>7.46</td>\n",
       "      <td>9.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01</th>\n",
       "      <td>122.480003</td>\n",
       "      <td>35.180000</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.18</td>\n",
       "      <td>16.230000</td>\n",
       "      <td>14.38</td>\n",
       "      <td>7.03</td>\n",
       "      <td>106.480003</td>\n",
       "      <td>73.720001</td>\n",
       "      <td>195.610001</td>\n",
       "      <td>...</td>\n",
       "      <td>26.920000</td>\n",
       "      <td>185.509995</td>\n",
       "      <td>18.780001</td>\n",
       "      <td>11.76</td>\n",
       "      <td>6.88</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.90</td>\n",
       "      <td>30.410000</td>\n",
       "      <td>7.45</td>\n",
       "      <td>8.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-02</th>\n",
       "      <td>127.709999</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>10.63</td>\n",
       "      <td>10.18</td>\n",
       "      <td>15.950000</td>\n",
       "      <td>14.23</td>\n",
       "      <td>7.39</td>\n",
       "      <td>106.540001</td>\n",
       "      <td>69.639999</td>\n",
       "      <td>192.580002</td>\n",
       "      <td>...</td>\n",
       "      <td>26.620001</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>18.780001</td>\n",
       "      <td>10.68</td>\n",
       "      <td>6.53</td>\n",
       "      <td>2.62</td>\n",
       "      <td>4.81</td>\n",
       "      <td>30.379999</td>\n",
       "      <td>7.32</td>\n",
       "      <td>8.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5933 rows × 3978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     A         AA    AAC   AACT        AAL    AAN  AAOI  \\\n",
       "2000-01-03   44.290768  71.203278    NaN    NaN        NaN    NaN   NaN   \n",
       "2000-01-04   40.907444  71.533180    NaN    NaN        NaN    NaN   NaN   \n",
       "2000-01-05   38.369949  75.656914    NaN    NaN        NaN    NaN   NaN   \n",
       "2000-01-06   36.908974  74.667213    NaN    NaN        NaN    NaN   NaN   \n",
       "2000-01-07   39.984726  74.447304    NaN    NaN        NaN    NaN   NaN   \n",
       "...                ...        ...    ...    ...        ...    ...   ...   \n",
       "2023-07-27  126.660004  33.910000  10.59  10.19  16.570000  15.38  6.19   \n",
       "2023-07-28  126.050003  34.759998  10.62  10.18  16.790001  15.43  6.71   \n",
       "2023-07-31  121.769997  36.189999  10.65  10.19  16.750000  15.82  6.75   \n",
       "2023-08-01  122.480003  35.180000  10.65  10.18  16.230000  14.38  7.03   \n",
       "2023-08-02  127.709999  33.930000  10.63  10.18  15.950000  14.23  7.39   \n",
       "\n",
       "                  AAON        AAP        AAPL  ...        ZTO         ZTS  \\\n",
       "2000-01-03    0.997502        NaN    0.859423  ...        NaN         NaN   \n",
       "2000-01-04    1.039065        NaN    0.786965  ...        NaN         NaN   \n",
       "2000-01-05    1.048301        NaN    0.798481  ...        NaN         NaN   \n",
       "2000-01-06    1.052919        NaN    0.729382  ...        NaN         NaN   \n",
       "2000-01-07    1.015974        NaN    0.763932  ...        NaN         NaN   \n",
       "...                ...        ...         ...  ...        ...         ...   \n",
       "2023-07-27  100.019997  72.639999  193.220001  ...  26.570000  188.690002   \n",
       "2023-07-28  103.019997  73.169998  195.830002  ...  27.990000  189.899994   \n",
       "2023-07-31  105.260002  74.389999  196.449997  ...  27.799999  188.089996   \n",
       "2023-08-01  106.480003  73.720001  195.610001  ...  26.920000  185.509995   \n",
       "2023-08-02  106.540001  69.639999  192.580002  ...  26.620001  182.000000   \n",
       "\n",
       "                 ZUMZ    ZUO  ZURA  ZVIA  ZVRA        ZWS  ZYME  ZYXI  \n",
       "2000-01-03        NaN    NaN   NaN   NaN   NaN        NaN   NaN   NaN  \n",
       "2000-01-04        NaN    NaN   NaN   NaN   NaN        NaN   NaN   NaN  \n",
       "2000-01-05        NaN    NaN   NaN   NaN   NaN        NaN   NaN   NaN  \n",
       "2000-01-06        NaN    NaN   NaN   NaN   NaN        NaN   NaN   NaN  \n",
       "2000-01-07        NaN    NaN   NaN   NaN   NaN        NaN   NaN   NaN  \n",
       "...               ...    ...   ...   ...   ...        ...   ...   ...  \n",
       "2023-07-27  18.760000  10.51  6.57  2.72  4.64  29.790001  7.28  9.75  \n",
       "2023-07-28  18.920000  10.91  7.00  2.72  4.83  30.440001  7.27  9.89  \n",
       "2023-07-31  18.860001  11.73  6.88  2.78  4.88  30.440001  7.46  9.76  \n",
       "2023-08-01  18.780001  11.76  6.88  2.75  4.90  30.410000  7.45  8.62  \n",
       "2023-08-02  18.780001  10.68  6.53  2.62  4.81  30.379999  7.32  8.29  \n",
       "\n",
       "[5933 rows x 3978 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fulldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d42f1f93",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDX</th>\n",
       "      <th>GDXJ</th>\n",
       "      <th>IBB</th>\n",
       "      <th>ITB</th>\n",
       "      <th>IYE</th>\n",
       "      <th>IYR</th>\n",
       "      <th>KBE</th>\n",
       "      <th>KRE</th>\n",
       "      <th>OIH</th>\n",
       "      <th>SMH</th>\n",
       "      <th>...</th>\n",
       "      <th>XLI</th>\n",
       "      <th>XLK</th>\n",
       "      <th>XLP</th>\n",
       "      <th>XLU</th>\n",
       "      <th>XLV</th>\n",
       "      <th>XLY</th>\n",
       "      <th>XME</th>\n",
       "      <th>XOP</th>\n",
       "      <th>XRT</th>\n",
       "      <th>XTL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>18.826677</td>\n",
       "      <td>42.028301</td>\n",
       "      <td>13.078809</td>\n",
       "      <td>11.990412</td>\n",
       "      <td>21.399870</td>\n",
       "      <td>22.808760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>18.306538</td>\n",
       "      <td>39.896088</td>\n",
       "      <td>12.710517</td>\n",
       "      <td>11.628716</td>\n",
       "      <td>20.914268</td>\n",
       "      <td>22.122601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>18.224949</td>\n",
       "      <td>39.303822</td>\n",
       "      <td>12.935083</td>\n",
       "      <td>11.922166</td>\n",
       "      <td>20.726645</td>\n",
       "      <td>21.850506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>18.469727</td>\n",
       "      <td>38.000782</td>\n",
       "      <td>13.177616</td>\n",
       "      <td>11.901689</td>\n",
       "      <td>20.792860</td>\n",
       "      <td>22.098946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19.173431</td>\n",
       "      <td>38.664139</td>\n",
       "      <td>14.013000</td>\n",
       "      <td>12.004058</td>\n",
       "      <td>21.046705</td>\n",
       "      <td>23.140007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-27</th>\n",
       "      <td>30.350000</td>\n",
       "      <td>36.099998</td>\n",
       "      <td>127.910004</td>\n",
       "      <td>87.809998</td>\n",
       "      <td>45.240002</td>\n",
       "      <td>87.720001</td>\n",
       "      <td>41.439999</td>\n",
       "      <td>48.029999</td>\n",
       "      <td>332.619995</td>\n",
       "      <td>157.460007</td>\n",
       "      <td>...</td>\n",
       "      <td>109.480003</td>\n",
       "      <td>175.550003</td>\n",
       "      <td>75.320000</td>\n",
       "      <td>67.190002</td>\n",
       "      <td>134.789993</td>\n",
       "      <td>169.880005</td>\n",
       "      <td>52.220001</td>\n",
       "      <td>138.139999</td>\n",
       "      <td>66.419998</td>\n",
       "      <td>73.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-28</th>\n",
       "      <td>30.719999</td>\n",
       "      <td>36.549999</td>\n",
       "      <td>129.860001</td>\n",
       "      <td>88.940002</td>\n",
       "      <td>45.360001</td>\n",
       "      <td>87.550003</td>\n",
       "      <td>41.939999</td>\n",
       "      <td>48.720001</td>\n",
       "      <td>337.500000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>110.160004</td>\n",
       "      <td>177.940002</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>67.040001</td>\n",
       "      <td>135.190002</td>\n",
       "      <td>172.759995</td>\n",
       "      <td>52.669998</td>\n",
       "      <td>140.919998</td>\n",
       "      <td>67.199997</td>\n",
       "      <td>73.199997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-31</th>\n",
       "      <td>31.410000</td>\n",
       "      <td>37.660000</td>\n",
       "      <td>129.309998</td>\n",
       "      <td>88.769997</td>\n",
       "      <td>46.230000</td>\n",
       "      <td>88.050003</td>\n",
       "      <td>41.959999</td>\n",
       "      <td>48.689999</td>\n",
       "      <td>343.739990</td>\n",
       "      <td>160.619995</td>\n",
       "      <td>...</td>\n",
       "      <td>110.419998</td>\n",
       "      <td>178.350006</td>\n",
       "      <td>75.750000</td>\n",
       "      <td>67.070000</td>\n",
       "      <td>134.149994</td>\n",
       "      <td>173.740005</td>\n",
       "      <td>53.889999</td>\n",
       "      <td>143.029999</td>\n",
       "      <td>67.779999</td>\n",
       "      <td>73.519997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01</th>\n",
       "      <td>30.230000</td>\n",
       "      <td>36.290001</td>\n",
       "      <td>128.050003</td>\n",
       "      <td>89.309998</td>\n",
       "      <td>45.990002</td>\n",
       "      <td>87.879997</td>\n",
       "      <td>41.540001</td>\n",
       "      <td>48.169998</td>\n",
       "      <td>341.700012</td>\n",
       "      <td>160.529999</td>\n",
       "      <td>...</td>\n",
       "      <td>110.750000</td>\n",
       "      <td>178.649994</td>\n",
       "      <td>75.339996</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>133.449997</td>\n",
       "      <td>171.770004</td>\n",
       "      <td>53.290001</td>\n",
       "      <td>142.559998</td>\n",
       "      <td>67.440002</td>\n",
       "      <td>73.830002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-02</th>\n",
       "      <td>29.280001</td>\n",
       "      <td>35.139999</td>\n",
       "      <td>127.360001</td>\n",
       "      <td>88.320000</td>\n",
       "      <td>45.320000</td>\n",
       "      <td>87.370003</td>\n",
       "      <td>41.320000</td>\n",
       "      <td>47.950001</td>\n",
       "      <td>339.250000</td>\n",
       "      <td>154.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>109.589996</td>\n",
       "      <td>174.229996</td>\n",
       "      <td>75.610001</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>133.539993</td>\n",
       "      <td>168.690002</td>\n",
       "      <td>51.750000</td>\n",
       "      <td>140.240005</td>\n",
       "      <td>66.809998</td>\n",
       "      <td>72.980003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5933 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  GDX       GDXJ         IBB        ITB        IYE        IYR  \\\n",
       "2000-01-03        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "2000-01-04        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "2000-01-05        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "2000-01-06        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "2000-01-07        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "...               ...        ...         ...        ...        ...        ...   \n",
       "2023-07-27  30.350000  36.099998  127.910004  87.809998  45.240002  87.720001   \n",
       "2023-07-28  30.719999  36.549999  129.860001  88.940002  45.360001  87.550003   \n",
       "2023-07-31  31.410000  37.660000  129.309998  88.769997  46.230000  88.050003   \n",
       "2023-08-01  30.230000  36.290001  128.050003  89.309998  45.990002  87.879997   \n",
       "2023-08-02  29.280001  35.139999  127.360001  88.320000  45.320000  87.370003   \n",
       "\n",
       "                  KBE        KRE         OIH         SMH  ...         XLI  \\\n",
       "2000-01-03        NaN        NaN         NaN         NaN  ...   18.826677   \n",
       "2000-01-04        NaN        NaN         NaN         NaN  ...   18.306538   \n",
       "2000-01-05        NaN        NaN         NaN         NaN  ...   18.224949   \n",
       "2000-01-06        NaN        NaN         NaN         NaN  ...   18.469727   \n",
       "2000-01-07        NaN        NaN         NaN         NaN  ...   19.173431   \n",
       "...               ...        ...         ...         ...  ...         ...   \n",
       "2023-07-27  41.439999  48.029999  332.619995  157.460007  ...  109.480003   \n",
       "2023-07-28  41.939999  48.720001  337.500000  160.500000  ...  110.160004   \n",
       "2023-07-31  41.959999  48.689999  343.739990  160.619995  ...  110.419998   \n",
       "2023-08-01  41.540001  48.169998  341.700012  160.529999  ...  110.750000   \n",
       "2023-08-02  41.320000  47.950001  339.250000  154.639999  ...  109.589996   \n",
       "\n",
       "                   XLK        XLP        XLU         XLV         XLY  \\\n",
       "2000-01-03   42.028301  13.078809  11.990412   21.399870   22.808760   \n",
       "2000-01-04   39.896088  12.710517  11.628716   20.914268   22.122601   \n",
       "2000-01-05   39.303822  12.935083  11.922166   20.726645   21.850506   \n",
       "2000-01-06   38.000782  13.177616  11.901689   20.792860   22.098946   \n",
       "2000-01-07   38.664139  14.013000  12.004058   21.046705   23.140007   \n",
       "...                ...        ...        ...         ...         ...   \n",
       "2023-07-27  175.550003  75.320000  67.190002  134.789993  169.880005   \n",
       "2023-07-28  177.940002  76.099998  67.040001  135.190002  172.759995   \n",
       "2023-07-31  178.350006  75.750000  67.070000  134.149994  173.740005   \n",
       "2023-08-01  178.649994  75.339996  66.250000  133.449997  171.770004   \n",
       "2023-08-02  174.229996  75.610001  66.250000  133.539993  168.690002   \n",
       "\n",
       "                  XME         XOP        XRT        XTL  \n",
       "2000-01-03        NaN         NaN        NaN        NaN  \n",
       "2000-01-04        NaN         NaN        NaN        NaN  \n",
       "2000-01-05        NaN         NaN        NaN        NaN  \n",
       "2000-01-06        NaN         NaN        NaN        NaN  \n",
       "2000-01-07        NaN         NaN        NaN        NaN  \n",
       "...               ...         ...        ...        ...  \n",
       "2023-07-27  52.220001  138.139999  66.419998  73.680000  \n",
       "2023-07-28  52.669998  140.919998  67.199997  73.199997  \n",
       "2023-07-31  53.889999  143.029999  67.779999  73.519997  \n",
       "2023-08-01  53.290001  142.559998  67.440002  73.830002  \n",
       "2023-08-02  51.750000  140.240005  66.809998  72.980003  \n",
       "\n",
       "[5933 rows x 25 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etfdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c901eb15",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                   GDX       GDXJ         IBB        ITB        IYE        IYR  \\\n",
       "dates                                                                           \n",
       "2000-01-03        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "2000-01-04        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "2000-01-05        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "2000-01-06        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "2000-01-07        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "...               ...        ...         ...        ...        ...        ...   \n",
       "2023-07-27  30.350000  36.099998  127.910004  87.809998  45.240002  87.720001   \n",
       "2023-07-28  30.719999  36.549999  129.860001  88.940002  45.360001  87.550003   \n",
       "2023-07-31  31.410000  37.660000  129.309998  88.769997  46.230000  88.050003   \n",
       "2023-08-01  30.230000  36.290001  128.050003  89.309998  45.990002  87.879997   \n",
       "2023-08-02  29.280001  35.139999  127.360001  88.320000  45.320000  87.370003   \n",
       "\n",
       "                  KBE        KRE         OIH         SMH  ...        ZTO  \\\n",
       "dates                                                     ...              \n",
       "2000-01-03        NaN        NaN         NaN         NaN  ...        NaN   \n",
       "2000-01-04        NaN        NaN         NaN         NaN  ...        NaN   \n",
       "2000-01-05        NaN        NaN         NaN         NaN  ...        NaN   \n",
       "2000-01-06        NaN        NaN         NaN         NaN  ...        NaN   \n",
       "2000-01-07        NaN        NaN         NaN         NaN  ...        NaN   \n",
       "...               ...        ...         ...         ...  ...        ...   \n",
       "2023-07-27  41.439999  48.029999  332.619995  157.460007  ...  26.570000   \n",
       "2023-07-28  41.939999  48.720001  337.500000  160.500000  ...  27.990000   \n",
       "2023-07-31  41.959999  48.689999  343.739990  160.619995  ...  27.799999   \n",
       "2023-08-01  41.540001  48.169998  341.700012  160.529999  ...  26.920000   \n",
       "2023-08-02  41.320000  47.950001  339.250000  154.639999  ...  26.620001   \n",
       "\n",
       "                   ZTS       ZUMZ    ZUO  ZURA  ZVIA  ZVRA        ZWS  ZYME  \\\n",
       "dates                                                                         \n",
       "2000-01-03         NaN        NaN    NaN   NaN   NaN   NaN        NaN   NaN   \n",
       "2000-01-04         NaN        NaN    NaN   NaN   NaN   NaN        NaN   NaN   \n",
       "2000-01-05         NaN        NaN    NaN   NaN   NaN   NaN        NaN   NaN   \n",
       "2000-01-06         NaN        NaN    NaN   NaN   NaN   NaN        NaN   NaN   \n",
       "2000-01-07         NaN        NaN    NaN   NaN   NaN   NaN        NaN   NaN   \n",
       "...                ...        ...    ...   ...   ...   ...        ...   ...   \n",
       "2023-07-27  188.690002  18.760000  10.51  6.57  2.72  4.64  29.790001  7.28   \n",
       "2023-07-28  189.899994  18.920000  10.91  7.00  2.72  4.83  30.440001  7.27   \n",
       "2023-07-31  188.089996  18.860001  11.73  6.88  2.78  4.88  30.440001  7.46   \n",
       "2023-08-01  185.509995  18.780001  11.76  6.88  2.75  4.90  30.410000  7.45   \n",
       "2023-08-02  182.000000  18.780001  10.68  6.53  2.62  4.81  30.379999  7.32   \n",
       "\n",
       "            ZYXI  \n",
       "dates             \n",
       "2000-01-03   NaN  \n",
       "2000-01-04   NaN  \n",
       "2000-01-05   NaN  \n",
       "2000-01-06   NaN  \n",
       "2000-01-07   NaN  \n",
       "...          ...  \n",
       "2023-07-27  9.75  \n",
       "2023-07-28  9.89  \n",
       "2023-07-31  9.76  \n",
       "2023-08-01  8.62  \n",
       "2023-08-02  8.29  \n",
       "\n",
       "[5933 rows x 4003 columns]>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data = etfdata.merge(fulldata, left_index=True, right_index=True)\n",
    "merged_data = merged_data.rename_axis('dates')\n",
    "merged_data.to_csv('merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56d8072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('merged_data.csv', index_col='dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d4ab019a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDX</th>\n",
       "      <th>GDXJ</th>\n",
       "      <th>IBB</th>\n",
       "      <th>ITB</th>\n",
       "      <th>IYE</th>\n",
       "      <th>IYR</th>\n",
       "      <th>KBE</th>\n",
       "      <th>KRE</th>\n",
       "      <th>OIH</th>\n",
       "      <th>SMH</th>\n",
       "      <th>...</th>\n",
       "      <th>ZTO</th>\n",
       "      <th>ZTS</th>\n",
       "      <th>ZUMZ</th>\n",
       "      <th>ZUO</th>\n",
       "      <th>ZURA</th>\n",
       "      <th>ZVIA</th>\n",
       "      <th>ZVRA</th>\n",
       "      <th>ZWS</th>\n",
       "      <th>ZYME</th>\n",
       "      <th>ZYXI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-27</th>\n",
       "      <td>30.350000</td>\n",
       "      <td>36.099998</td>\n",
       "      <td>127.910004</td>\n",
       "      <td>87.809998</td>\n",
       "      <td>45.240002</td>\n",
       "      <td>87.720001</td>\n",
       "      <td>41.439999</td>\n",
       "      <td>48.029999</td>\n",
       "      <td>332.619995</td>\n",
       "      <td>157.460007</td>\n",
       "      <td>...</td>\n",
       "      <td>26.570000</td>\n",
       "      <td>188.690002</td>\n",
       "      <td>18.760000</td>\n",
       "      <td>10.51</td>\n",
       "      <td>6.57</td>\n",
       "      <td>2.72</td>\n",
       "      <td>4.64</td>\n",
       "      <td>29.790001</td>\n",
       "      <td>7.28</td>\n",
       "      <td>9.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-28</th>\n",
       "      <td>30.719999</td>\n",
       "      <td>36.549999</td>\n",
       "      <td>129.860001</td>\n",
       "      <td>88.940002</td>\n",
       "      <td>45.360001</td>\n",
       "      <td>87.550003</td>\n",
       "      <td>41.939999</td>\n",
       "      <td>48.720001</td>\n",
       "      <td>337.500000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>27.990000</td>\n",
       "      <td>189.899994</td>\n",
       "      <td>18.920000</td>\n",
       "      <td>10.91</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2.72</td>\n",
       "      <td>4.83</td>\n",
       "      <td>30.440001</td>\n",
       "      <td>7.27</td>\n",
       "      <td>9.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-31</th>\n",
       "      <td>31.410000</td>\n",
       "      <td>37.660000</td>\n",
       "      <td>129.309998</td>\n",
       "      <td>88.769997</td>\n",
       "      <td>46.230000</td>\n",
       "      <td>88.050003</td>\n",
       "      <td>41.959999</td>\n",
       "      <td>48.689999</td>\n",
       "      <td>343.739990</td>\n",
       "      <td>160.619995</td>\n",
       "      <td>...</td>\n",
       "      <td>27.799999</td>\n",
       "      <td>188.089996</td>\n",
       "      <td>18.860001</td>\n",
       "      <td>11.73</td>\n",
       "      <td>6.88</td>\n",
       "      <td>2.78</td>\n",
       "      <td>4.88</td>\n",
       "      <td>30.440001</td>\n",
       "      <td>7.46</td>\n",
       "      <td>9.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01</th>\n",
       "      <td>30.230000</td>\n",
       "      <td>36.290001</td>\n",
       "      <td>128.050003</td>\n",
       "      <td>89.309998</td>\n",
       "      <td>45.990002</td>\n",
       "      <td>87.879997</td>\n",
       "      <td>41.540001</td>\n",
       "      <td>48.169998</td>\n",
       "      <td>341.700012</td>\n",
       "      <td>160.529999</td>\n",
       "      <td>...</td>\n",
       "      <td>26.920000</td>\n",
       "      <td>185.509995</td>\n",
       "      <td>18.780001</td>\n",
       "      <td>11.76</td>\n",
       "      <td>6.88</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.90</td>\n",
       "      <td>30.410000</td>\n",
       "      <td>7.45</td>\n",
       "      <td>8.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-02</th>\n",
       "      <td>29.280001</td>\n",
       "      <td>35.139999</td>\n",
       "      <td>127.360001</td>\n",
       "      <td>88.320000</td>\n",
       "      <td>45.320000</td>\n",
       "      <td>87.370003</td>\n",
       "      <td>41.320000</td>\n",
       "      <td>47.950001</td>\n",
       "      <td>339.250000</td>\n",
       "      <td>154.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>26.620001</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>18.780001</td>\n",
       "      <td>10.68</td>\n",
       "      <td>6.53</td>\n",
       "      <td>2.62</td>\n",
       "      <td>4.81</td>\n",
       "      <td>30.379999</td>\n",
       "      <td>7.32</td>\n",
       "      <td>8.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5933 rows × 4003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  GDX       GDXJ         IBB        ITB        IYE        IYR  \\\n",
       "dates                                                                           \n",
       "2000-01-03        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "2000-01-04        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "2000-01-05        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "2000-01-06        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "2000-01-07        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "...               ...        ...         ...        ...        ...        ...   \n",
       "2023-07-27  30.350000  36.099998  127.910004  87.809998  45.240002  87.720001   \n",
       "2023-07-28  30.719999  36.549999  129.860001  88.940002  45.360001  87.550003   \n",
       "2023-07-31  31.410000  37.660000  129.309998  88.769997  46.230000  88.050003   \n",
       "2023-08-01  30.230000  36.290001  128.050003  89.309998  45.990002  87.879997   \n",
       "2023-08-02  29.280001  35.139999  127.360001  88.320000  45.320000  87.370003   \n",
       "\n",
       "                  KBE        KRE         OIH         SMH  ...        ZTO  \\\n",
       "dates                                                     ...              \n",
       "2000-01-03        NaN        NaN         NaN         NaN  ...        NaN   \n",
       "2000-01-04        NaN        NaN         NaN         NaN  ...        NaN   \n",
       "2000-01-05        NaN        NaN         NaN         NaN  ...        NaN   \n",
       "2000-01-06        NaN        NaN         NaN         NaN  ...        NaN   \n",
       "2000-01-07        NaN        NaN         NaN         NaN  ...        NaN   \n",
       "...               ...        ...         ...         ...  ...        ...   \n",
       "2023-07-27  41.439999  48.029999  332.619995  157.460007  ...  26.570000   \n",
       "2023-07-28  41.939999  48.720001  337.500000  160.500000  ...  27.990000   \n",
       "2023-07-31  41.959999  48.689999  343.739990  160.619995  ...  27.799999   \n",
       "2023-08-01  41.540001  48.169998  341.700012  160.529999  ...  26.920000   \n",
       "2023-08-02  41.320000  47.950001  339.250000  154.639999  ...  26.620001   \n",
       "\n",
       "                   ZTS       ZUMZ    ZUO  ZURA  ZVIA  ZVRA        ZWS  ZYME  \\\n",
       "dates                                                                         \n",
       "2000-01-03         NaN        NaN    NaN   NaN   NaN   NaN        NaN   NaN   \n",
       "2000-01-04         NaN        NaN    NaN   NaN   NaN   NaN        NaN   NaN   \n",
       "2000-01-05         NaN        NaN    NaN   NaN   NaN   NaN        NaN   NaN   \n",
       "2000-01-06         NaN        NaN    NaN   NaN   NaN   NaN        NaN   NaN   \n",
       "2000-01-07         NaN        NaN    NaN   NaN   NaN   NaN        NaN   NaN   \n",
       "...                ...        ...    ...   ...   ...   ...        ...   ...   \n",
       "2023-07-27  188.690002  18.760000  10.51  6.57  2.72  4.64  29.790001  7.28   \n",
       "2023-07-28  189.899994  18.920000  10.91  7.00  2.72  4.83  30.440001  7.27   \n",
       "2023-07-31  188.089996  18.860001  11.73  6.88  2.78  4.88  30.440001  7.46   \n",
       "2023-08-01  185.509995  18.780001  11.76  6.88  2.75  4.90  30.410000  7.45   \n",
       "2023-08-02  182.000000  18.780001  10.68  6.53  2.62  4.81  30.379999  7.32   \n",
       "\n",
       "            ZYXI  \n",
       "dates             \n",
       "2000-01-03   NaN  \n",
       "2000-01-04   NaN  \n",
       "2000-01-05   NaN  \n",
       "2000-01-06   NaN  \n",
       "2000-01-07   NaN  \n",
       "...          ...  \n",
       "2023-07-27  9.75  \n",
       "2023-07-28  9.89  \n",
       "2023-07-31  9.76  \n",
       "2023-08-01  8.62  \n",
       "2023-08-02  8.29  \n",
       "\n",
       "[5933 rows x 4003 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4310dc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2000-01-01 00:00:00'),\n",
       " Timestamp('2001-01-01 00:00:00'),\n",
       " Timestamp('2002-01-01 00:00:00'),\n",
       " Timestamp('2003-01-01 00:00:00'),\n",
       " Timestamp('2004-01-01 00:00:00'),\n",
       " Timestamp('2005-01-01 00:00:00'),\n",
       " Timestamp('2006-01-01 00:00:00'),\n",
       " Timestamp('2007-01-01 00:00:00'),\n",
       " Timestamp('2008-01-01 00:00:00'),\n",
       " Timestamp('2009-01-01 00:00:00'),\n",
       " Timestamp('2010-01-01 00:00:00'),\n",
       " Timestamp('2011-01-01 00:00:00'),\n",
       " Timestamp('2012-01-01 00:00:00'),\n",
       " Timestamp('2013-01-01 00:00:00'),\n",
       " Timestamp('2014-01-01 00:00:00'),\n",
       " Timestamp('2015-01-01 00:00:00'),\n",
       " Timestamp('2016-01-01 00:00:00'),\n",
       " Timestamp('2017-01-01 00:00:00'),\n",
       " Timestamp('2018-01-01 00:00:00'),\n",
       " Timestamp('2019-01-01 00:00:00'),\n",
       " Timestamp('2020-01-01 00:00:00'),\n",
       " Timestamp('2021-01-01 00:00:00'),\n",
       " Timestamp('2022-01-01 00:00:00'),\n",
       " Timestamp('2023-01-01 00:00:00')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Define the start and end dates of the entire period\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2023-08-02'\n",
    "\n",
    "# Generate a list of semester start and end dates\n",
    "semester_dates = []\n",
    "current_date = pd.Timestamp(start_date)\n",
    "while current_date < pd.Timestamp(end_date):\n",
    "    semester_dates.append(current_date.normalize())\n",
    "    if current_date.month == 1:\n",
    "        current_date += pd.DateOffset(months=12)\n",
    "    else:\n",
    "        current_date += pd.DateOffset(months=12)\n",
    "semester_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7abac03f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semester_1: 126 rows\n",
      "Semester_2: 126 rows\n",
      "Semester_3: 125 rows\n",
      "Semester_4: 123 rows\n",
      "Semester_5: 124 rows\n",
      "Semester_6: 128 rows\n",
      "Semester_7: 124 rows\n",
      "Semester_8: 128 rows\n",
      "Semester_9: 124 rows\n",
      "Semester_10: 128 rows\n",
      "Semester_11: 125 rows\n",
      "Semester_12: 127 rows\n",
      "Semester_13: 125 rows\n",
      "Semester_14: 126 rows\n",
      "Semester_15: 124 rows\n",
      "Semester_16: 127 rows\n",
      "Semester_17: 125 rows\n",
      "Semester_18: 128 rows\n",
      "Semester_19: 124 rows\n",
      "Semester_20: 128 rows\n",
      "Semester_21: 124 rows\n",
      "Semester_22: 128 rows\n",
      "Semester_23: 125 rows\n",
      "Semester_24: 127 rows\n",
      "Semester_25: 125 rows\n",
      "Semester_26: 125 rows\n",
      "Semester_27: 124 rows\n",
      "Semester_28: 128 rows\n",
      "Semester_29: 124 rows\n",
      "Semester_30: 128 rows\n",
      "Semester_31: 124 rows\n",
      "Semester_32: 128 rows\n",
      "Semester_33: 125 rows\n",
      "Semester_34: 127 rows\n",
      "Semester_35: 125 rows\n",
      "Semester_36: 126 rows\n",
      "Semester_37: 125 rows\n",
      "Semester_38: 126 rows\n",
      "Semester_39: 124 rows\n",
      "Semester_40: 128 rows\n",
      "Semester_41: 125 rows\n",
      "Semester_42: 128 rows\n",
      "Semester_43: 124 rows\n",
      "Semester_44: 128 rows\n",
      "Semester_45: 124 rows\n",
      "Semester_46: 127 rows\n",
      "Semester_47: 124 rows\n",
      "Semester_48: 22 rows\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to store the semester DataFrames\n",
    "semester_dataframes = {}\n",
    "\n",
    "# Loop through the semester start dates\n",
    "for i in range(len(semester_dates) - 1):\n",
    "    # Get the start and end dates for the current semester\n",
    "    semester_start = semester_dates[i]\n",
    "    semester_end = semester_dates[i + 1]\n",
    "    \n",
    "    # Slice the original DataFrame for the current semester\n",
    "    semester_df = data[(data.index >= semester_start) & (data.index < semester_end)]\n",
    "    \n",
    "    # Store the semester DataFrame in the dictionary\n",
    "    semester_dataframes[f'Semester_{i + 1}'] = semester_df\n",
    "\n",
    "# Include the last semester separately (from the last semester_end to end_date)\n",
    "semester_df = data[(data.index >= semester_dates[-1]) & (data.index <= end_date)]\n",
    "semester_dataframes[f'Semester_{len(semester_dates)}'] = semester_df\n",
    "\n",
    "# Print the number of rows in each semester for verification\n",
    "for semester, df in semester_dataframes.items():\n",
    "    print(f\"{semester}: {len(df)} rows\")\n",
    "\n",
    "# Access individual semester DataFrames using semester_dataframes['Semester_1'], etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ed6ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('semester_dataframes.pkl', 'wb') as f:\n",
    "    pickle.dump(semester_dataframes, f)\n",
    "\n",
    "#with open('semester_dataframes.pkl', 'rb') as f:\n",
    "   # loaded_semester_dataframes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "efa19c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_semester_dataframes = {}\n",
    "\n",
    "# Loop through each semester DataFrame and create a new DataFrame without NaN columns\n",
    "for semester_name, semester_df in semester_dataframes.items():\n",
    "    valid_columns = semester_df.columns[~semester_df.isna().any()]\n",
    "    modified_semester_dataframes[semester_name] = semester_df[valid_columns]\n",
    "\n",
    "# Now 'modified_semester_dataframes' contains the modified DataFrames with columns without NaN values for each semester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d44f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separe data by semesters and drop na\n",
    "import pandas as pd\n",
    "# Define the start and end dates of the entire period\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2023-08-02'\n",
    "\n",
    "# Generate a list of semester start and end dates\n",
    "semester_dates = []\n",
    "current_date = pd.Timestamp(start_date)\n",
    "while current_date < pd.Timestamp(end_date):\n",
    "    semester_dates.append(current_date.normalize())\n",
    "    if current_date.month == 1:\n",
    "        current_date += pd.DateOffset(months=6)\n",
    "    else:\n",
    "        current_date += pd.DateOffset(months=6)\n",
    "# Initialize an empty dictionary to store the semester DataFrames\n",
    "semester_dataframes = {}\n",
    "\n",
    "# Loop through the semester start dates\n",
    "for i in range(len(semester_dates) - 1):\n",
    "    # Get the start and end dates for the current semester\n",
    "    semester_start = semester_dates[i]\n",
    "    semester_end = semester_dates[i + 1]\n",
    "    \n",
    "    # Slice the original DataFrame for the current semester\n",
    "    semester_df = open_data[(open_data.index >= semester_start) & (open_data.index < semester_end)]\n",
    "    \n",
    "    # Store the semester DataFrame in the dictionary\n",
    "    semester_dataframes[f'Semester_{i + 1}'] = semester_df\n",
    "\n",
    "# Include the last semester separately (from the last semester_end to end_date)\n",
    "semester_df = open_data[(open_data.index >= semester_dates[-1]) & (open_data.index <= end_date)]\n",
    "semester_dataframes[f'Semester_{len(semester_dates)}'] = semester_df\n",
    "\n",
    "# Print the number of rows in each semester for verification\n",
    "for semester, df in semester_dataframes.items():\n",
    "    print(f\"{semester}: {len(df)} rows\")\n",
    "\n",
    "for semester_name, semester_df in semester_dataframes.items():\n",
    "    valid_columns = semester_df.columns[~semester_df.isna().any()]\n",
    "    semester_dataframes[semester_name] = semester_df[valid_columns]\n",
    "# Create a new dictionary with updated keys\n",
    "open_bysemester = {}\n",
    "for key in semester_dataframes.keys():\n",
    "    new_key = key.replace('Semester_', 'Semester_')\n",
    "    open_bysemester[new_key] = semester_dataframes[key]\n",
    "\n",
    "    # drop every column with a 0\n",
    "for semester_name in open_bysemester.keys():\n",
    "    semester_df = open_bysemester[semester_name]\n",
    "    \n",
    "    # Identify columns with at least one value equal to 0\n",
    "    columns_to_drop = semester_df.columns[semester_df.eq(0).any()]\n",
    "    # Drop the identified columns from the DataFrame\n",
    "    open_bysemester[semester_name] = semester_df.drop(columns=columns_to_drop, inplace=False)\n",
    "    \n",
    "with open('open_bysemester.pkl', 'wb') as f:\n",
    "    pickle.dump(open_bysemester, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is called open_data\n",
    "yearly_dataframes = {}\n",
    "\n",
    "for year in range(2000, 2024):  # Assuming you want data from 2000 to 2023\n",
    "    start_date = pd.to_datetime(f\"{year}-01-01\")\n",
    "    end_date = pd.to_datetime(f\"{year}-12-31\")\n",
    "    \n",
    "    # Extract data for the current year\n",
    "    year_data = open_data[(open_data.index >= start_date) & (open_data.index <= end_date)]\n",
    "    \n",
    "    # Store the year's data in a dictionary with the year as the key\n",
    "    yearly_dataframes[year] = year_data\n",
    "\n",
    "# Access the yearly dataframes using the year as the key, e.g., yearly_dataframes[2022] for the year 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "\n",
    "years = sorted(set(yearly_dataframes.keys()) | set(yearlysem_dataframes.keys()))\n",
    "semesters = ['Semester_1', 'Semester_2']\n",
    "\n",
    "semester_index = 0\n",
    "\n",
    "for year in years:\n",
    "    result_dict[year] = {semesters[semester_index]: yearly_dataframes[year]}\n",
    "    semester_index = (semester_index + 1) % len(semesters)\n",
    "    result_dict[year].update({semesters[semester_index]: yearlysem_dataframes[year]})\n",
    "    semester_index = (semester_index + 1) % len(semesters)\n",
    "\n",
    "print(result_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of semesters you want (48 in this case)\n",
    "num_semesters = 48\n",
    "\n",
    "# Initialize the semester counter\n",
    "semester_count = 1\n",
    "\n",
    "# Create a new dictionary to store the result with the desired keys\n",
    "new_result_dict = {}\n",
    "\n",
    "# Iterate through the original result_dict and rename the keys\n",
    "for year in result_dict:\n",
    "    for old_key in list(result_dict[year].keys()):\n",
    "        new_key = f'Semester_{semester_count}'\n",
    "        new_result_dict[new_key] = result_dict[year].pop(old_key)\n",
    "        semester_count += 1\n",
    "        if semester_count > num_semesters:\n",
    "            break\n",
    "\n",
    "# Print the updated result_dict\n",
    "print(new_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9598b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "start_date = \"2000-07-01\"\n",
    "end_date = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\n",
    "risk_free_data = yf.download(\"^IRX\", start=start_date, end=end_date)\n",
    "risk_free_data = risk_free_data['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05890274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Define the start and end dates of the entire period\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2023-08-02'\n",
    "\n",
    "# Generate a list of semester start and end dates\n",
    "semester_dates = []\n",
    "current_date = pd.Timestamp(start_date)\n",
    "while current_date < pd.Timestamp(end_date):\n",
    "    semester_dates.append(current_date.normalize())\n",
    "    if current_date.month == 1:\n",
    "        current_date += pd.DateOffset(months=6)\n",
    "    else:\n",
    "        current_date += pd.DateOffset(months=6)\n",
    "semester_dates\n",
    "# Initialize an empty dictionary to store the semester DataFrames\n",
    "riskfree_bysemester = {}\n",
    "\n",
    "# Loop through the semester start dates\n",
    "for i in range(len(semester_dates) - 1):\n",
    "    # Convert semester_start and semester_end to Timestamp objects\n",
    "    semester_start = pd.to_datetime(semester_dates[i])\n",
    "    semester_end = pd.to_datetime(semester_dates[i + 1])\n",
    "    \n",
    "    # Ensure the risk_free_data index is in Timestamp format\n",
    "    risk_free_data.index = pd.to_datetime(risk_free_data.index)\n",
    "    \n",
    "    # Slice the original DataFrame for the current semester\n",
    "    semester_df = risk_free_data[(risk_free_data.index >= semester_start) & (risk_free_data.index < semester_end)]\n",
    "    \n",
    "    # Store the semester DataFrame in the riskfree_bysemester dictionary\n",
    "    riskfree_bysemester[f'Semester_{i + 1}'] = semester_df\n",
    "\n",
    "# Include the last semester separately (from the last semester_end to end_date)\n",
    "end_date = pd.to_datetime(end_date)  # Ensure end_date is in Timestamp format\n",
    "semester_df = risk_free_data[(risk_free_data.index >= pd.to_datetime(semester_dates[-1])) & (risk_free_data.index <= end_date)]\n",
    "riskfree_bysemester[f'Semester_{len(semester_dates)}'] = semester_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a0721a4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XLB</th>\n",
       "      <th>XLE</th>\n",
       "      <th>XLF</th>\n",
       "      <th>XLI</th>\n",
       "      <th>XLK</th>\n",
       "      <th>XLP</th>\n",
       "      <th>XLU</th>\n",
       "      <th>XLV</th>\n",
       "      <th>XLY</th>\n",
       "      <th>A</th>\n",
       "      <th>...</th>\n",
       "      <th>PACW</th>\n",
       "      <th>PKG</th>\n",
       "      <th>PUK</th>\n",
       "      <th>RBBN</th>\n",
       "      <th>SGMO</th>\n",
       "      <th>SLAB</th>\n",
       "      <th>SLF</th>\n",
       "      <th>SMMF</th>\n",
       "      <th>UBS</th>\n",
       "      <th>UONEK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>15.257446</td>\n",
       "      <td>14.549553</td>\n",
       "      <td>11.589029</td>\n",
       "      <td>18.826677</td>\n",
       "      <td>42.028301</td>\n",
       "      <td>13.078809</td>\n",
       "      <td>11.990412</td>\n",
       "      <td>21.399870</td>\n",
       "      <td>22.808760</td>\n",
       "      <td>44.290768</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>15.110559</td>\n",
       "      <td>14.275674</td>\n",
       "      <td>11.082405</td>\n",
       "      <td>18.306538</td>\n",
       "      <td>39.896088</td>\n",
       "      <td>12.710517</td>\n",
       "      <td>11.628716</td>\n",
       "      <td>20.914268</td>\n",
       "      <td>22.122601</td>\n",
       "      <td>40.907444</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>15.734811</td>\n",
       "      <td>14.652250</td>\n",
       "      <td>10.995327</td>\n",
       "      <td>18.224949</td>\n",
       "      <td>39.303822</td>\n",
       "      <td>12.935083</td>\n",
       "      <td>11.922166</td>\n",
       "      <td>20.726645</td>\n",
       "      <td>21.850506</td>\n",
       "      <td>38.369949</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>16.138741</td>\n",
       "      <td>15.217112</td>\n",
       "      <td>11.478209</td>\n",
       "      <td>18.469727</td>\n",
       "      <td>38.000782</td>\n",
       "      <td>13.177616</td>\n",
       "      <td>11.901689</td>\n",
       "      <td>20.792860</td>\n",
       "      <td>22.098946</td>\n",
       "      <td>36.908974</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>16.129557</td>\n",
       "      <td>15.379725</td>\n",
       "      <td>11.668194</td>\n",
       "      <td>19.173431</td>\n",
       "      <td>38.664139</td>\n",
       "      <td>14.013000</td>\n",
       "      <td>12.004058</td>\n",
       "      <td>21.046705</td>\n",
       "      <td>23.140007</td>\n",
       "      <td>39.984726</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-22</th>\n",
       "      <td>13.043835</td>\n",
       "      <td>17.801161</td>\n",
       "      <td>14.824342</td>\n",
       "      <td>19.901163</td>\n",
       "      <td>24.816593</td>\n",
       "      <td>16.004091</td>\n",
       "      <td>14.592863</td>\n",
       "      <td>19.041483</td>\n",
       "      <td>18.714567</td>\n",
       "      <td>33.640987</td>\n",
       "      <td>...</td>\n",
       "      <td>7.692776</td>\n",
       "      <td>8.578258</td>\n",
       "      <td>12.767458</td>\n",
       "      <td>135.0000</td>\n",
       "      <td>14.750</td>\n",
       "      <td>11.875</td>\n",
       "      <td>10.436805</td>\n",
       "      <td>3.023864</td>\n",
       "      <td>12.749737</td>\n",
       "      <td>10.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-26</th>\n",
       "      <td>13.005663</td>\n",
       "      <td>18.548674</td>\n",
       "      <td>15.089202</td>\n",
       "      <td>20.128006</td>\n",
       "      <td>24.401997</td>\n",
       "      <td>16.322346</td>\n",
       "      <td>15.003925</td>\n",
       "      <td>19.074606</td>\n",
       "      <td>18.356512</td>\n",
       "      <td>32.910496</td>\n",
       "      <td>...</td>\n",
       "      <td>7.725236</td>\n",
       "      <td>8.612298</td>\n",
       "      <td>12.565601</td>\n",
       "      <td>126.2500</td>\n",
       "      <td>18.875</td>\n",
       "      <td>11.500</td>\n",
       "      <td>10.327229</td>\n",
       "      <td>3.023864</td>\n",
       "      <td>12.775508</td>\n",
       "      <td>10.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-27</th>\n",
       "      <td>13.215588</td>\n",
       "      <td>18.531281</td>\n",
       "      <td>15.185520</td>\n",
       "      <td>20.457977</td>\n",
       "      <td>24.638914</td>\n",
       "      <td>16.431471</td>\n",
       "      <td>15.003925</td>\n",
       "      <td>19.140833</td>\n",
       "      <td>19.239723</td>\n",
       "      <td>34.409939</td>\n",
       "      <td>...</td>\n",
       "      <td>7.855070</td>\n",
       "      <td>9.088867</td>\n",
       "      <td>12.893622</td>\n",
       "      <td>125.0000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>13.625</td>\n",
       "      <td>11.176416</td>\n",
       "      <td>3.023864</td>\n",
       "      <td>12.925296</td>\n",
       "      <td>10.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-28</th>\n",
       "      <td>13.339633</td>\n",
       "      <td>18.835501</td>\n",
       "      <td>15.338017</td>\n",
       "      <td>20.757017</td>\n",
       "      <td>24.520456</td>\n",
       "      <td>16.676991</td>\n",
       "      <td>14.883445</td>\n",
       "      <td>19.471985</td>\n",
       "      <td>19.478430</td>\n",
       "      <td>34.025459</td>\n",
       "      <td>...</td>\n",
       "      <td>7.692776</td>\n",
       "      <td>8.986743</td>\n",
       "      <td>13.019787</td>\n",
       "      <td>134.6875</td>\n",
       "      <td>21.875</td>\n",
       "      <td>13.500</td>\n",
       "      <td>11.477743</td>\n",
       "      <td>2.981866</td>\n",
       "      <td>12.920467</td>\n",
       "      <td>10.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-29</th>\n",
       "      <td>13.081999</td>\n",
       "      <td>18.461750</td>\n",
       "      <td>15.153411</td>\n",
       "      <td>20.622971</td>\n",
       "      <td>23.738642</td>\n",
       "      <td>16.622429</td>\n",
       "      <td>15.053540</td>\n",
       "      <td>19.251225</td>\n",
       "      <td>19.550037</td>\n",
       "      <td>33.679428</td>\n",
       "      <td>...</td>\n",
       "      <td>7.692776</td>\n",
       "      <td>8.782498</td>\n",
       "      <td>12.868394</td>\n",
       "      <td>126.2500</td>\n",
       "      <td>19.500</td>\n",
       "      <td>14.375</td>\n",
       "      <td>11.669497</td>\n",
       "      <td>2.981866</td>\n",
       "      <td>13.158838</td>\n",
       "      <td>11.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 1460 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  XLB        XLE        XLF        XLI        XLK        XLP  \\\n",
       "dates                                                                          \n",
       "2000-01-03  15.257446  14.549553  11.589029  18.826677  42.028301  13.078809   \n",
       "2000-01-04  15.110559  14.275674  11.082405  18.306538  39.896088  12.710517   \n",
       "2000-01-05  15.734811  14.652250  10.995327  18.224949  39.303822  12.935083   \n",
       "2000-01-06  16.138741  15.217112  11.478209  18.469727  38.000782  13.177616   \n",
       "2000-01-07  16.129557  15.379725  11.668194  19.173431  38.664139  14.013000   \n",
       "...               ...        ...        ...        ...        ...        ...   \n",
       "2000-12-22  13.043835  17.801161  14.824342  19.901163  24.816593  16.004091   \n",
       "2000-12-26  13.005663  18.548674  15.089202  20.128006  24.401997  16.322346   \n",
       "2000-12-27  13.215588  18.531281  15.185520  20.457977  24.638914  16.431471   \n",
       "2000-12-28  13.339633  18.835501  15.338017  20.757017  24.520456  16.676991   \n",
       "2000-12-29  13.081999  18.461750  15.153411  20.622971  23.738642  16.622429   \n",
       "\n",
       "                  XLU        XLV        XLY          A  ...      PACW  \\\n",
       "dates                                                   ...             \n",
       "2000-01-03  11.990412  21.399870  22.808760  44.290768  ...       NaN   \n",
       "2000-01-04  11.628716  20.914268  22.122601  40.907444  ...       NaN   \n",
       "2000-01-05  11.922166  20.726645  21.850506  38.369949  ...       NaN   \n",
       "2000-01-06  11.901689  20.792860  22.098946  36.908974  ...       NaN   \n",
       "2000-01-07  12.004058  21.046705  23.140007  39.984726  ...       NaN   \n",
       "...               ...        ...        ...        ...  ...       ...   \n",
       "2000-12-22  14.592863  19.041483  18.714567  33.640987  ...  7.692776   \n",
       "2000-12-26  15.003925  19.074606  18.356512  32.910496  ...  7.725236   \n",
       "2000-12-27  15.003925  19.140833  19.239723  34.409939  ...  7.855070   \n",
       "2000-12-28  14.883445  19.471985  19.478430  34.025459  ...  7.692776   \n",
       "2000-12-29  15.053540  19.251225  19.550037  33.679428  ...  7.692776   \n",
       "\n",
       "                 PKG        PUK      RBBN    SGMO    SLAB        SLF  \\\n",
       "dates                                                                  \n",
       "2000-01-03       NaN        NaN       NaN     NaN     NaN        NaN   \n",
       "2000-01-04       NaN        NaN       NaN     NaN     NaN        NaN   \n",
       "2000-01-05       NaN        NaN       NaN     NaN     NaN        NaN   \n",
       "2000-01-06       NaN        NaN       NaN     NaN     NaN        NaN   \n",
       "2000-01-07       NaN        NaN       NaN     NaN     NaN        NaN   \n",
       "...              ...        ...       ...     ...     ...        ...   \n",
       "2000-12-22  8.578258  12.767458  135.0000  14.750  11.875  10.436805   \n",
       "2000-12-26  8.612298  12.565601  126.2500  18.875  11.500  10.327229   \n",
       "2000-12-27  9.088867  12.893622  125.0000  21.000  13.625  11.176416   \n",
       "2000-12-28  8.986743  13.019787  134.6875  21.875  13.500  11.477743   \n",
       "2000-12-29  8.782498  12.868394  126.2500  19.500  14.375  11.669497   \n",
       "\n",
       "                SMMF        UBS    UONEK  \n",
       "dates                                     \n",
       "2000-01-03       NaN        NaN      NaN  \n",
       "2000-01-04       NaN        NaN      NaN  \n",
       "2000-01-05       NaN        NaN      NaN  \n",
       "2000-01-06       NaN        NaN      NaN  \n",
       "2000-01-07       NaN        NaN      NaN  \n",
       "...              ...        ...      ...  \n",
       "2000-12-22  3.023864  12.749737  10.0625  \n",
       "2000-12-26  3.023864  12.775508  10.5000  \n",
       "2000-12-27  3.023864  12.925296  10.6250  \n",
       "2000-12-28  2.981866  12.920467  10.2500  \n",
       "2000-12-29  2.981866  13.158838  11.0000  \n",
       "\n",
       "[252 rows x 1460 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debugging version\n",
    "class OpticsPairs:\n",
    "    \"\"\"\n",
    "    This class implements the pairs selection framework outlined in\n",
    "    Simao Moraes Saremtno and Nuno Horta's publication:\n",
    "    Enhancing a Pairs Trading strategy with the application\n",
    "    of Machine Learning [1].\n",
    "    <http://premio-vidigal.inesc.pt/pdf/SimaoSarmentoMSc-resumo.pdf>`_\n",
    "\n",
    "    Their work is motivated by the need to find \"profitable pairs while\n",
    "    constraining the search space\" [1]. To achieve this, security returns\n",
    "    are first reduced via principal component analysis. Next the securities are\n",
    "    paired through clustering via the OPTICS algorithim introduced by\n",
    "    Ankerst et. al in their publication: OPTICS: Ordering Points To Identify\n",
    "    the Clustering Structure [2]\n",
    "    <https://www.dbs.ifi.lmu.de/Publikationen/Papers/OPTICS.pdf>`_\n",
    "    Finally, the pairs are filtered by criteria including: the Engle-Granger\n",
    "    test, analysis of the Hurst exponent, half-life filtering, and practical\n",
    "    implementation requirements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initializes OpticsPairs object and calculates one-period returns of\n",
    "        securities.\n",
    "\n",
    "        :param data: pd.DataFrame containing time series returns of various\n",
    "            assets. Dimensions of dataframe should be TxN.\n",
    "        \"\"\"\n",
    "\n",
    "        self.prices = data\n",
    "        self.securities = self.prices.columns\n",
    "        self.returns = self.prices.pct_change()[1:]\n",
    "        self.returns_reduced = None  # Reduced transform of returns from PCA\n",
    "        self.components_ = None  # Components generated from PCA\n",
    "        self.n_components_ = None  # Number of components of PCA\n",
    "        self.explained_variance_ratio_ = None  # Vairance explained by PCA\n",
    "        self.pairs = None  # Potential pairs found from OPTICS clusters\n",
    "        self.engle_granger_tests = None  # pvalue Engle-Granger cointegration\n",
    "        self.norm_spreads = None  # Z-score of spreads generated from pairs\n",
    "        self.hurst_exponents = None  # Hurst exponent  from normalized spreads\n",
    "        self.half_lives = None  # Half-life of normalized spreads\n",
    "        self.avg_cross_count = None  # Ann average count of spread crosses mean\n",
    "        self.pairs_df = None  # Dataframeof summary stats and potential pairs\n",
    "        self.filtered_pairs = None  # Filtered pairs_df\n",
    "        self.cluster_labels = None  # Array of cluster labels for securities\n",
    "\n",
    "    def reduce_PCA(self,\n",
    "                   n_components_: int = 10,\n",
    "                   Scaler=StandardScaler(),\n",
    "                   random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Reduces self.returns to dimensions equal to n_components_ through\n",
    "        principal component analysis. Returns are first scaled via the Scaler\n",
    "        parameter. Then calculate correlation matrix of scaled returns.\n",
    "        Finally, principal component analysis is used to reduce dimensions.\n",
    "\n",
    "        :param n_components_: An integer to denote number of dimensions\n",
    "            for pca. Authors recommend n_components_ <= 15 [1].\n",
    "        :param Scaler: A transformer to scale input data. Scaled data is\n",
    "            recommended for principal component analysis.\n",
    "        :param random_state: An integer to denote the seed for PCA() to insure\n",
    "            reproducibility.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.returns is None:\n",
    "            raise ValueError(\"returns not found: input price dataframe \\\n",
    "                             into OpticsPairs instance\")\n",
    "\n",
    "        if n_components_ > int(15):\n",
    "            warnings.warn(\"Maximum n_components_ recommended is 15\")\n",
    "\n",
    "        # PCA pipeline\n",
    "        pipe = Pipeline([\n",
    "            # Normalize raw data via user input scaler\n",
    "            ('scaler', Scaler),\n",
    "            # Perform PCA on scaled returns\n",
    "            ('pca', PCA(n_components=n_components_, random_state=random_state))\n",
    "            ])\n",
    "\n",
    "        self.returns_reduced = pipe.fit_transform(self.returns)\n",
    "        self.components_ = pipe['pca'].components_\n",
    "        self.n_components_ = pipe['pca'].n_components_\n",
    "        self.explained_variance_ratio_ = pipe['pca'].explained_variance_ratio_\n",
    "\n",
    "    def find_pairs(self):\n",
    "        \"\"\"\n",
    "        Uses OPTICS algorithim to find clusters of similar securities within\n",
    "        PCA component space. Once clusters labels are assigned, function\n",
    "        generates series of tuples containing unique pairs of securities\n",
    "        within the same cluster.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.returns_reduced is None:\n",
    "            raise ValueError(\"returns_reduced not found: must run \\\n",
    "                             .reduce_PCA() before this function\")\n",
    "\n",
    "        # Initialize and fit OPTICS cluster to PCA components\n",
    "        clustering = OPTICS()\n",
    "        clustering.fit(self.components_.T)\n",
    "\n",
    "        # Create cluster data frame and identify trading pairs\n",
    "        clusters = pd.DataFrame({'security': self.securities,\n",
    "                                 'cluster': clustering.labels_})\n",
    "        # Clusters with label == -1 are 'noise'\n",
    "        # From OPTICS sk-learn documentation: Noisy samples and points\n",
    "        # which are not included in a leaf cluster of cluster_hierarchy_\n",
    "        # are labeled as -1\n",
    "        clusters = clusters[clusters['cluster'] != -1]\n",
    "\n",
    "        # Group securities by cluster and flatten list of combination lists\n",
    "        groups = clusters.groupby('cluster')\n",
    "        combos = list(groups['security'].apply(combinations, 2))  # All pairs\n",
    "        pairs = list(chain.from_iterable(combos))  # Flatten list of lists\n",
    "\n",
    "        print(f\"Found {len(pairs)} potential pairs\")\n",
    "\n",
    "        self.pairs = pd.Series(pairs)\n",
    "        self.cluster_labels = clustering.labels_\n",
    "\n",
    "    def calc_eg_norm_spreads(self):\n",
    "        \"\"\"\n",
    "        Calculates the p-value of the t-stat from the Engle-Granger\n",
    "        cointegration test. Calculates normalized beta-adjusted spread\n",
    "        series of potential pairs.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.prices is None:\n",
    "            raise ValueError(\"prices not found: must initialize with \\\n",
    "                             price dataframe before this function\")\n",
    "\n",
    "        if self.pairs is None:\n",
    "            raise ValueError(\"pairs not found: must run .find_pairs() \\\n",
    "                             before this function\")\n",
    "\n",
    "        engle_granger_tests = []\n",
    "        norm_spreads = []\n",
    "\n",
    "        # Test each pair for cointegration\n",
    "        for pair in self.pairs:\n",
    "\n",
    "            security_0 = self.prices[pair[0]]\n",
    "            security_1 = self.prices[pair[1]]\n",
    "\n",
    "            # Get independent and dependent variables\n",
    "            # for OLS calculation and corresponding\n",
    "            # pvalue for Engle-Granger tests\n",
    "            pvalue, x, y = OpticsPairs.get_ols_variables(security_0, security_1)\n",
    "            engle_granger_tests.append(pvalue)\n",
    "\n",
    "            try:\n",
    "                # Get parameters and calculate spread\n",
    "                model = sm.OLS(y, x)\n",
    "                result = model.fit()\n",
    "                alpha, beta = result.params[0], result.params[1]\n",
    "                spread = y - (alpha + beta * x[:, 1])\n",
    "            except (IndexError, np.linalg.LinAlgError) as e:\n",
    "                print(f\"Error occurred for pair {pair}: {e}\")\n",
    "                beta = 0  # Set a default value in case of an error\n",
    "                spread = y - alpha\n",
    "    \n",
    "            norm_spread = OpticsPairs.calc_zscore(spread)\n",
    "            norm_spreads.append(norm_spread)\n",
    "\n",
    "        # Convert spreads from list to dataframe\n",
    "        norm_spreads = pd.DataFrame(np.transpose(norm_spreads),\n",
    "                                    index=self.prices.index)\n",
    "\n",
    "        self.norm_spreads = norm_spreads\n",
    "        self.engle_granger_tests = pd.Series(engle_granger_tests)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_ols_variables(security_0: str,\n",
    "                          security_1: str):\n",
    "        \"\"\"\n",
    "        Compares t-stats of two Engle-Granger cointegration tests.\n",
    "        Returns independent and dependent variables for OLS.\n",
    "\n",
    "        :params security_0: String identifier of first security.\n",
    "        :params security_1: String identifier of second security.\n",
    "        \"\"\"\n",
    "\n",
    "        test_0 = ts.coint(security_0, security_1)\n",
    "        test_1 = ts.coint(security_1, security_0)\n",
    "\n",
    "        t_stat_0, pvalue_0 = test_0[0], test_0[1]\n",
    "        t_stat_1, pvalue_1 = test_1[0], test_1[1]\n",
    "          \n",
    "        # Avoid reliance on dependent variable and choose smallest t-stat\n",
    "        # for Engle-Granger Test\n",
    "        # Use corresponding independent and dependent variables to\n",
    "        # calculate spread\n",
    "        if abs(t_stat_0) < abs(t_stat_1):\n",
    "            pvalue = pvalue_0\n",
    "            x = sm.add_constant(np.asarray(security_1))\n",
    "            y = np.asarray(security_0)\n",
    "        else:\n",
    "            pvalue = pvalue_1\n",
    "            x = sm.add_constant(np.asarray(security_0))\n",
    "            y = np.asarray(security_1)\n",
    "\n",
    "        return pvalue, x, y\n",
    "        \n",
    "    def calc_hurst_exponents(self):\n",
    "        \"\"\"\n",
    "        Calculates Hurst exponent of each potential pair's normalized spread.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.norm_spreads is None:\n",
    "            raise ValueError(\"norm_spreads not found: must run \\\n",
    "                            .calc_eg_norm_spreads before this function\")\n",
    "\n",
    "        hurst_exponents = []\n",
    "\n",
    "        # Calculate Hurst exponents and generate series\n",
    "        for col in self.norm_spreads.columns:\n",
    "            hurst_exp = OpticsPairs.hurst(self.norm_spreads[col].values)\n",
    "            hurst_exponents.append(hurst_exp)\n",
    "\n",
    "        self.hurst_exponents = pd.Series(hurst_exponents)\n",
    "\n",
    "    def calc_half_lives(self):\n",
    "        \"\"\"\n",
    "        Calculates half-life of each potential pair's normalized spread.\n",
    "        \"\"\"\n",
    "        if self.norm_spreads is None:\n",
    "            raise ValueError(\"norm_spreads not found: must run \\\n",
    "                            .calc_eg_norm_spreads before this function\")\n",
    "\n",
    "        self.half_lives = self.norm_spreads.apply(OpticsPairs.half_life)\n",
    "\n",
    "    def calc_avg_cross_count(self, trading_year: float = 252.0):\n",
    "        \"\"\"\n",
    "        Calculates the average number of instances per year the\n",
    "        normalized spread of potential pairs crosses the mean.\n",
    "        Authors recommend trading pairs that cross mean on average\n",
    "        12 times per year [1].\n",
    "        \"\"\"\n",
    "\n",
    "        if self.prices is None:\n",
    "            raise ValueError(\"prices not found: must initialize with \\\n",
    "                                price dataframe before this function\")\n",
    "\n",
    "        if self.norm_spreads is None:\n",
    "            raise ValueError(\"norm_spreads not found: must run \\\n",
    "                            .calc_eg_norm_spreads() before this function\")\n",
    "\n",
    "        # Find number of years\n",
    "        n_days = len(self.prices)\n",
    "        n_years = n_days/trading_year\n",
    "\n",
    "        # Find annual average cross count\n",
    "        cross_count = self.norm_spreads.apply(OpticsPairs.count_crosses)\n",
    "        self.avg_cross_count = cross_count/n_years\n",
    "\n",
    "    def filter_pairs(self,\n",
    "                     max_pvalue: float = 0.05,\n",
    "                     max_hurst_exp: float = 0.5,\n",
    "                     max_half_life: float = 252.0,\n",
    "                     min_half_life: float = 1.0,\n",
    "                     min_avg_cross: float = 12.0):\n",
    "        \"\"\"\n",
    "        Generates a summary dataframe of potential pairs containing:\n",
    "            1. Engle-Granger p-value\n",
    "            2. Hurst exponent\n",
    "            3. Half-life\n",
    "            4. Average Cross Count\n",
    "        Filters summary dataframe to include pairs that meet user\n",
    "        specified criteria.\n",
    "\n",
    "        :param max_pvalue: A floating number to eliminate potential pairs with\n",
    "            Engle-Granger t-stat pvalues above max_pvalue. Default set to 5%.\n",
    "        :param max_hurst_exp: A floating number to eliminate potential\n",
    "            pairs with Hurst exponents greater than max_hurst_exp.\n",
    "            Values below 0.5 represent mean-reverting pairs.\n",
    "            Default set to 0.5.\n",
    "        :param max_half_life: A floating number to eliminate potential pairs\n",
    "            with half-lives above user defined value.\n",
    "            Default value set to 252.0.\n",
    "        :param min_half_life: A floating number to eliminate potential\n",
    "            pairs with half-lives below user defined value.\n",
    "            Default value set to 1.0.\n",
    "        :min_avg_cross: A floating number to eliminate potential pairs with\n",
    "            average cross count less than user defined value.\n",
    "            Default value set to 12.0\n",
    "        \"\"\"\n",
    "\n",
    "        required = [self.prices,\n",
    "                    self.engle_granger_tests,\n",
    "                    self.hurst_exponents,\n",
    "                    self.half_lives,\n",
    "                    self.avg_cross_count]\n",
    "\n",
    "        for i in required:\n",
    "            if i is None:\n",
    "                raise ValueError(\"Required: \\n 1. prices \\n 2. \\\n",
    "                                engle_granger_tests \\n 3. hurst_exponents \\\n",
    "                                \\n 4. half_lives \\n 5. avg_cross_count\")\n",
    "\n",
    "        # Generate summary dataframe of potential trading pairs\n",
    "        pairs_df = pd.concat([self.pairs,\n",
    "                              self.engle_granger_tests,\n",
    "                              self.hurst_exponents,\n",
    "                              self.half_lives,\n",
    "                              self.avg_cross_count],\n",
    "                             axis=1)\n",
    "        pairs_df.columns = ['pair',\n",
    "                            'pvalue',\n",
    "                            'hurst_exp',\n",
    "                            'half_life',\n",
    "                            'avg_cross_count']\n",
    "\n",
    "        # Find pairs that meet user defined criteria\n",
    "        filtered_pairs = pairs_df.loc[\n",
    "            # Significant Engle-Grange test AND\n",
    "            (pairs_df['pvalue'] <= max_pvalue) &\n",
    "            # Mean reverting according to Hurst exponent AND\n",
    "            (pairs_df['hurst_exp'] < max_hurst_exp) &\n",
    "            # Half-life above minimum value AND\n",
    "            # Half-life below maximum value AND\n",
    "            ((pairs_df['half_life'] >= min_half_life) &\n",
    "             (pairs_df['half_life'] <= max_half_life)) &\n",
    "            # Produces sufficient number of trading opportunities\n",
    "            (pairs_df['avg_cross_count'] >= min_avg_cross)]\n",
    "\n",
    "        self.pairs_df = pairs_df\n",
    "        self.filtered_pairs = filtered_pairs\n",
    "\n",
    "        if len(self.filtered_pairs) == 0:\n",
    "            print(\"No tradable pairs found. Try relaxing criteria.\")\n",
    "        else:\n",
    "            n_pairs = len(self.filtered_pairs)\n",
    "            print(f\"Found {n_pairs} tradable pairs!\")\n",
    "\n",
    "    def plot_pair_price_spread(self, idx: int):\n",
    "        \"\"\"\n",
    "        Plots the price path of both securities in selected pair,\n",
    "        with dual axis. Plots the normalized spread of the price paths.\n",
    "        \"\"\"\n",
    "        required = [self.prices,\n",
    "                    self.pairs,\n",
    "                    self.norm_spreads,\n",
    "                    self.half_lives,\n",
    "                    self.avg_cross_count]\n",
    "\n",
    "        for i in required:\n",
    "            if i is None:\n",
    "                raise ValueError(\"Required: \\n 1. prices \\n 2. pairs \\\n",
    "                                \\n 3. norm_spreads\")\n",
    "\n",
    "        fontsize = 20\n",
    "        securities = self.pairs[idx]\n",
    "\n",
    "        fig, axs = plt.subplots(2, 1, sharex=True, figsize=(20, 10))\n",
    "\n",
    "        # first security (left axis)\n",
    "        security = securities[0]\n",
    "        color = 'tab:blue'\n",
    "        axs[0].plot(self.prices[security], color=color)\n",
    "        axs[0].set_ylabel(security, color=color, fontsize=fontsize)\n",
    "        axs[0].tick_params(axis='y', labelcolor=color)\n",
    "        axs[0].set_title('pair_'+str(idx)+' prices', fontsize=fontsize)\n",
    "\n",
    "        # second security (right axis)\n",
    "        security = securities[1]\n",
    "        color = 'tab:orange'\n",
    "        axs2 = axs[0].twinx()\n",
    "        axs2.plot(self.prices[security], color=color)\n",
    "        axs2.set_ylabel(security, color=color, fontsize=fontsize)\n",
    "        axs2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        # plot spread\n",
    "        axs[1].plot(self.norm_spreads[idx], color='black')\n",
    "        axs[1].set_ylabel('spread_z_score', fontsize=fontsize)\n",
    "        axs[1].set_xlabel('date', fontsize=fontsize)\n",
    "        axs[1].set_title('pair_'+str(idx)+' normalized spread',\n",
    "                         fontsize=fontsize)\n",
    "        axs[1].axhline(0, color='blue', ls='--')\n",
    "        axs[1].axhline(1, color='r', ls='--')\n",
    "        axs[1].axhline(-1, color='r', ls='--')\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "    def plot_explained_variance(self):\n",
    "        \"\"\"\n",
    "        Plots the cumulative variance explained by principal component\n",
    "        analysis.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.explained_variance_ratio_ is None:\n",
    "            raise ValueError(\"explained_variance_ratio_ missing: run \\\n",
    "                            .reduce_PCA() before this function\")\n",
    "\n",
    "        fig, axs = plt.subplots()\n",
    "        axs.set_title('PCA Cumulative Explained Variance')\n",
    "        axs.plot(np.cumsum(self.explained_variance_ratio_))\n",
    "        axs.set_xlabel('number of components')\n",
    "        axs.set_ylabel('explained variance')\n",
    "        fig.tight_layout()\n",
    "\n",
    "    def plot_loadings(self, n: int = 5):\n",
    "        \"\"\"\n",
    "        Plots up to 5 bar charts depicting the loadings of\n",
    "        each component, by security.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.components_ is None:\n",
    "            raise ValueError(\"components_ missing: run \\\n",
    "                            .reduce_PCA() before this function\")\n",
    "\n",
    "        n_loadings = min(n, self.n_components_)\n",
    "        fig, axs = plt.subplots(n_loadings, 1, sharex=True, figsize=(20, 20))\n",
    "        fontsize = 18\n",
    "        for i in range(n_loadings):\n",
    "            axs[i].bar([i for i in range(self.components_.shape[1])],\n",
    "                       self.components_[i])\n",
    "            axs[i].set_ylabel('component_'+str(i)+' loading',\n",
    "                              fontsize=fontsize)\n",
    "        axs[0].set_title('PCA Loadings', fontsize=fontsize)\n",
    "        axs[i].set_xlabel('security_observation', fontsize=fontsize)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "    def plot_clusters(self, n_dimensions: int = 2):\n",
    "        \"\"\"\n",
    "        Plots a 2-dimension or 3-dimension scatter plot of security principal\n",
    "        component loadings. Plots either the first two or three\n",
    "        principal components and colors securities according to their\n",
    "        cluster label found from OPTICS algorithm.\n",
    "\n",
    "        :param n_dimensions: An integer to denote how many dimensions to plot.\n",
    "            Default value is two.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in [self.n_components_, self.components_, self.cluster_labels]:\n",
    "            if i is None:\n",
    "                raise ValueError(\"Required: \\n 1. n_components \\n 2. \\\n",
    "                                reduced_returns\")\n",
    "\n",
    "        fontsize = 15\n",
    "        figsize = (10, 10)\n",
    "\n",
    "        if n_dimensions == 2:\n",
    "            fig, axs = plt.subplots(1, 1, figsize=figsize)\n",
    "            axs.scatter(self.components_[0].T[self.cluster_labels != -1],\n",
    "                        self.components_[1].T[self.cluster_labels != -1],\n",
    "                        c=self.cluster_labels[self.cluster_labels != -1])\n",
    "            axs.scatter(self.components_[0].T[self.cluster_labels == -1],\n",
    "                        self.components_[1].T[self.cluster_labels == -1],\n",
    "                        c=self.cluster_labels[self.cluster_labels == -1],\n",
    "                        alpha=0.1)\n",
    "            axs.set_title('OPTICS Clusters', fontsize=fontsize)\n",
    "            axs.set_xlabel('component_0 loading', fontsize=fontsize)\n",
    "            axs.set_ylabel('component_1 loading', fontsize=fontsize)\n",
    "            fig.tight_layout()\n",
    "\n",
    "        elif n_dimensions == 3:\n",
    "            fig = plt.figure(figsize=figsize)\n",
    "            axs = fig.add_subplot(111, projection='3d')\n",
    "            axs.scatter(self.components_[0].T[self.cluster_labels != -1],\n",
    "                        self.components_[1].T[self.cluster_labels != -1],\n",
    "                        self.components_[2].T[self.cluster_labels != -1],\n",
    "                        c=self.cluster_labels[self.cluster_labels != -1])\n",
    "            axs.scatter(self.components_[0].T[self.cluster_labels == -1],\n",
    "                        self.components_[1].T[self.cluster_labels == -1],\n",
    "                        self.components_[2].T[self.cluster_labels == -1],\n",
    "                        c=self.cluster_labels[self.cluster_labels == -1],\n",
    "                        alpha=0.1)\n",
    "            \n",
    "            axs.set_title('OPTICS Clusters', fontsize=fontsize)\n",
    "            axs.set_xlabel('component_0 loading', fontsize=fontsize)\n",
    "            axs.set_ylabel('component_1 loading', fontsize=fontsize)\n",
    "            axs.set_zlabel('component_2 loading', fontsize=fontsize)\n",
    "            fig.tight_layout()\n",
    "\n",
    "        else:\n",
    "            warnings.warn(\"Cannot visualize more than three dimensions!\")\n",
    "\n",
    "    @staticmethod\n",
    "    def hurst(norm_spread):\n",
    "        \"\"\"\n",
    "        Calculates Hurst exponent.\n",
    "        https://en.wikipedia.org/wiki/Hurst_exponent\n",
    "\n",
    "        :param norm_spread: An array like object used to calculate half-life.\n",
    "        \"\"\"\n",
    "        # Create the range of lag values\n",
    "        lags = range(2, 100)\n",
    "\n",
    "        # Calculate the array of the variances of the lagged differences\n",
    "        diffs = [np.subtract(norm_spread[l:], norm_spread[:-l]) for l in lags]\n",
    "        tau = [np.sqrt(np.std(diff)) for diff in diffs]\n",
    "        \n",
    "        # Use a linear fit to estimate the Hurst Exponent\n",
    "        poly = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "\n",
    "        # Return the Hurst exponent from the polyfit output\n",
    "        H = poly[0]*2.0\n",
    "\n",
    "        return H\n",
    "\n",
    "    @staticmethod\n",
    "    def half_life(norm_spread):\n",
    "        \"\"\"\n",
    "        Calculates time series half-life.\n",
    "        https://en.wikipedia.org/wiki/Half-life\n",
    "\n",
    "        :param norm_spread: An array like object used to calculate half-life.\n",
    "        \"\"\"\n",
    "        lag = norm_spread.shift(1)\n",
    "        lag[0] = lag[1]\n",
    "\n",
    "        ret = norm_spread - lag\n",
    "        lag = sm.add_constant(lag)\n",
    "\n",
    "        model = sm.OLS(ret, lag)\n",
    "        result = model.fit()\n",
    "        half_life = -np.log(2)/result.params.iloc[1]\n",
    "\n",
    "        return half_life\n",
    "\n",
    "    @staticmethod\n",
    "    def count_crosses(norm_spread, mean: float = 0.0):\n",
    "        \"\"\"\n",
    "        Calculates the number of times a time series crosses its mean.\n",
    "\n",
    "        :param norm_spread: An array like object used to calculate half-life.\n",
    "        :param mean: A float to denote mean of norm_spread.\n",
    "            Default value is 0.0.\n",
    "        \"\"\"\n",
    "\n",
    "        curr_period = norm_spread\n",
    "        next_period = norm_spread.shift(-1)\n",
    "        count = (\n",
    "            ((curr_period >= mean) & (next_period < mean)) |  # Over to under\n",
    "            ((curr_period < mean) & (next_period >= mean)) |  # Under to over\n",
    "            (curr_period == mean)\n",
    "            ).sum()\n",
    "\n",
    "        return count\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_zscore(spread):\n",
    "        zscore = (spread - np.mean(spread))/np.std(spread)\n",
    "        return zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ee1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b262740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
